{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository Name: Resume Recommender\n",
    "\n",
    "#### Description:\n",
    "Resume Recommender Python Notebook powered by Large Language Models (LLM) via Azure OpenAI! This notebook demonstrates how to utilize state-of-the-art natural language processing techniques to recommend resumes for specific job requirements. Leveraging Azure's OpenAI framework, this project aims to streamline the recruitment process by accurately matching candidates to job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dependencies \n",
    "\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings, AzureOpenAI\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API endpoint, version, type, key, and deployment model\n",
    "API_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "API_TYPE = os.getenv(\"OPENAI_API_TYPE\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DEPLOYMENT_MODEL = \"gpt-35-turbo-16k\"\n",
    "COMPLETIONS_MODEL = \"gpt-3.5-turbo-16k\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "TEMP = 0.3  # temperature \n",
    "\n",
    "# Initialize the AzureChatOpenAI object for language generation\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=API_ENDPOINT,\n",
    "    openai_api_version=API_VERSION,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=API_TYPE,\n",
    "    deployment_name=DEPLOYMENT_MODEL,\n",
    "    model_name=DEPLOYMENT_MODEL,\n",
    "    tiktoken_model_name=COMPLETIONS_MODEL,\n",
    "    temperature=TEMP\n",
    ")\n",
    "\n",
    "# Initialize the AzureOpenAIEmbeddings object for text embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=API_ENDPOINT,\n",
    "    openai_api_version=API_VERSION,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_type=API_TYPE,\n",
    "    deployment=EMBEDDING_MODEL,\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(directory):\n",
    "  \"\"\"\n",
    "  Load documents from the specified directory using unstructured parser\n",
    "\n",
    "  Args:\n",
    "    directory (str): The directory path where the documents are located.\n",
    "\n",
    "  Returns:\n",
    "    list: A list of loaded documents.\n",
    "  \"\"\"\n",
    "  loader = DirectoryLoader(directory, show_progress=True, loader_kwargs={\"strategy\": \"hi_res\", \"mode\": \"elements\"})\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "directory = 'C:/github/Resume chatbot/test_data/'\n",
    "docs = load_docs(directory)\n",
    "print(f\"number of docs: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experimental part - in development\n",
    "This part is not completed yet, the idea is to store summeries of multiple resumes rather then create embedding for a complete document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_documents_content(documents):\n",
    "    \"\"\"\n",
    "    Concatenates the content of a list of document objects into a single string.\n",
    "\n",
    "    Args:\n",
    "        documents (list): A list of document objects.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated content of the documents.\n",
    "    \"\"\"\n",
    "    txt = ''\n",
    "    for doc in documents:\n",
    "        txt += f\"\\n {doc.page_content}\"\n",
    "    \n",
    "    return txt\n",
    "\n",
    "\n",
    "# Concatenate the content of the documents into a single string\n",
    "txt = concat_documents_content(docs)\n",
    "\n",
    "# Run the LLM\n",
    "prompt = \"the following text between the > and < signs is a complete resume text extracted from a document. You are an HR summarizer that should summerize the text and focus on the following key sub topics: Name, location, education, skills and tools, programming languages, and experience. you can add also additional information which will be a one line extra information from the document only that worth mentioning\"\n",
    "context = f\"> {txt} <\"\n",
    "txt_summary = llm.invoke(prompt + context)\n",
    "print(txt_summary.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---> end of experimental code <----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "## here we are using Azure OpenAI embeddings \n",
    "from langchain.vectorstores import Chroma\n",
    "vectordb = Chroma.from_documents(documents=docs, \n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)\n",
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None\n",
    "\n",
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embeddings)\n",
    "\n",
    "chromaretriever = vectordb.as_retriever(search_kwargs={\"k\": 2}) # by default search_type=\"similarity_score_threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the necessary libraries\n",
    "import chromaretriever\n",
    "\n",
    "# Define a function to retrieve the page content for a given query\n",
    "def retrieve_page_content(query):\n",
    "    \"\"\"\n",
    "    Retrieves the page content for a given query using chromaretriever.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: The page content of the retrieved document.\n",
    "    \"\"\"\n",
    "    # Invoke the chromaretriever to retrieve the documents\n",
    "    docs = chromaretriever.invoke(query)\n",
    "\n",
    "    # Return the page content of the first document\n",
    "    return docs[0].page_content\n",
    "\n",
    "# Example usage\n",
    "query = \"which candidate is good fit for data analysis using Tableau roles.\"\n",
    "page_content = retrieve_page_content(query)\n",
    "print(page_content)\n",
    "\n",
    "\n",
    "# Example #2 \n",
    "query = \"Give name of candidate who is good fit for a Data Analyst roles\"\n",
    "page_content = retrieve_page_content(query)\n",
    "print(page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=chromaretriever, \n",
    "                                  verbose=True,\n",
    "                                  return_source_documents=True)\n",
    "\n",
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    \"\"\"\n",
    "    Process the LLM response and print the result and sources.\n",
    "\n",
    "    Parameters:\n",
    "    llm_response (dict): The LLM response containing the result and source documents.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A complete example\n",
    "Here I'm going to create a complete example with a detailed prompt. \n",
    "To modify the scenario for every role, you should insert the job description of your case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# full example\n",
    "warning = \"We are looking at resumes of candidates for a position of data analyst and Business Intelligence (BI) developer. If you don't know the answer, just say that you don't know, don't try to make up an answer and do not mention skills which doesn't appear in the context!\"\n",
    "job_description = \"\"\"\n",
    "We are looking for candidates for a student type of position for a Business intelligence (BI) developer and Analyst to work with internal customers, to understand and articulate business processes, map them to business intelligence requirements, and implement them into indicators, reports, and dashboards.\n",
    "\n",
    "As a BI Developer and analyst you will design, build, and maintain reports and dashboards to monitor company, product, and business performance and KPI's.\n",
    "\n",
    "The BI developers work with other team members from across the world to develop BI solutions and provide support to the BI systems and ongoing analysis needs.\n",
    "This includes understanding the data and where to get it, extracting the data, processing it, optimizations, and turning data into insights and graphical representation in the BI platform (Tableau).\n",
    "\n",
    "Here are the main JOB Qualifications we mostly looking for in each candidate resume:\n",
    "1. Studies degree in the field of industrial engineering, information systems, and /or Statistics.\n",
    "2. Experience in databases and SQL\n",
    "3. Experience in business intelligence and data warehousing concepts and methodologies\n",
    "4. Excellent self-learning capabilities\n",
    "5. Strong written and verbal communication skills and customer engagement skills\n",
    "6. Experience in Python and software development - Advantage\n",
    "7. Familiarity with data mining, machine learning, and predictive analytics algorithms, methodologies, and approaches - Advantage\n",
    "8. Knowledge or experience with MS Excel, Tableau, Power BI - Advantage\n",
    "9. Experience with AWS and cloud-based solutions in general - Advantage\n",
    "10. Availability for 3 days a week\n",
    "\n",
    "\"\"\"\n",
    "question = warning+job_description + \" Based on the given job description and the context you are provided with, which is the actual resumes of candidates for this job.:  \"\n",
    "query = question + \" short list up to two resumes which are good fit based on match of: skills and tools, education and work experience mentioned in it. also you must provide the candidate name which usually will be mentioned in first line of pdf without subheading. next to each name mention the key reasons why you think he or she are the best fit. make sure not to list skills and information from the job description but from the resumes in the context. In case there are no good fit in the context resumes, list only one or mention that there are no good fit.\"\n",
    "\n",
    "# query = question + \"retrive the full document information of top 3 resumes which are good fit based on skills,education and work experience mwntioned in it? \"\n",
    "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
    "\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_doc = chromaretriever.invoke(query)\n",
    "print(resume_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_doc = resume_doc[1].page_content\n",
    "print(resume_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = vectordb.get()\n",
    "print(db_records['documents'][0]) # you may want to change the key as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
